# LLM Model PyTorch

è¿™æ˜¯ä¸€ä¸ªçµæ´»ä¸”é«˜æ•ˆçš„ PyTorch å®ç°çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åº“ã€‚è¯¥é¡¹ç›®æ—¨åœ¨æä¾›ä¸€ä¸ªç®€æ´çš„ä»£ç åº“ï¼Œç”¨äºè®­ç»ƒå’Œæ¨ç†ç°ä»£ Transformer æ¨¡å‹ï¼Œæ”¯æŒ **Mixture of Experts (MoE)**ã€**RoPE (YaRN/Dynamic)** ä»¥åŠ **GQA/MQA** ç­‰å‰æ²¿æŠ€æœ¯ã€‚

## âœ¨ æ ¸å¿ƒç‰¹æ€§ (Key Features)

åŸºäºæºä»£ç åˆ†æï¼Œæœ¬é¡¹ç›®åŒ…å«ä»¥ä¸‹æ ¸å¿ƒåŠŸèƒ½ï¼š

*   **å…ˆè¿›çš„æ³¨æ„åŠ›æœºåˆ¶**:

    *   æ”¯æŒ **GQA (Grouped Query Attention)** å’Œ **MQA (Multi-Query Attention)**ï¼Œç”± `num_key_value_heads` å‚æ•°æ§åˆ¶ã€‚
    *   åŸç”Ÿæ”¯æŒ PyTorch çš„ `F.scaled_dot_product_attention` (Flash Attention) ä»¥åŠ é€Ÿè®¡ç®—ã€‚
*   **ä½ç½®ç¼–ç  (Positional Embeddings)**:

    *   å®ç° **Rotary Position Embeddings (RoPE)**ã€‚
    *   æ”¯æŒ **YaRN (Yet another RoPE extension)** ç”¨äºé•¿ä¸Šä¸‹æ–‡å¤–æ¨ã€‚
    *   æ”¯æŒ **Dynamic NTK** ç¼©æ”¾ã€‚
*   **æ··åˆä¸“å®¶æ¨¡å‹ (MoE)**:

    *   æ”¯æŒ **Sparse MoE** æ¶æ„ã€‚
    *   æ”¯æŒ **Shared Experts**ï¼ˆå…±äº«ä¸“å®¶ï¼‰ä¸ Routed Expertsï¼ˆè·¯ç”±ä¸“å®¶ï¼‰ç»“åˆçš„æœºåˆ¶ã€‚
    *   åŒ…å«è´Ÿè½½å‡è¡¡è¾…åŠ©æŸå¤± (Auxiliary Loss) å’Œ Z-Lossã€‚
*   **å¤šæ¨¡æ€èƒ½åŠ› (VLM)**:

    *   æä¾› `VlmModel`ï¼Œæ”¯æŒè‡ªå®šä¹‰è§†è§‰å¡” (Vision Tower)ã€‚
    *   å®ç°å¤šæ¨¡æ€æŠ•å½±å±‚ (Multi-Modal Projector)ï¼Œæ”¯æŒ Patch Poolingã€‚
*   **æ¨ç†ä¼˜åŒ–**:

    *   å®Œæ•´çš„ **KV Cache** å®ç°ï¼Œæ”¯æŒé«˜æ•ˆçš„è‡ªå›å½’è§£ç ã€‚
    *   æ”¯æŒæ¢¯åº¦æ£€æŸ¥ç‚¹ (Gradient Checkpointing) ä»¥èŠ‚çœè®­ç»ƒæ˜¾å­˜ã€‚
 
*   **é…å¥—è®­ç»ƒå’Œæ¨ç†æ¡†æ¶**:
    *   [https://github.com/qibin0506/llm_trainer](https://github.com/qibin0506/llm_trainer)

## ğŸ› ï¸ å®‰è£… (Installation)

ä½ å¯ä»¥é€šè¿‡ pip ç›´æ¥å®‰è£…ï¼š

``` Bash
pip3 install project_llm_model

```

æˆ–è€…ä»æºç å®‰è£…ï¼š

``` Bash
git clone https://github.com/qibin0506/llm_model.git
cd llm_model
python3 setup.py install

```

## ğŸš€ å¿«é€Ÿå¼€å§‹ (Quick Start)

### 1. åˆå§‹åŒ– LLM æ¨¡å‹

``` Python
import torch
from llm_model import LlmModel, ModelConfig, RoPEConfig

# é…ç½®ä¸€ä¸ªç®€å•çš„ LLM
config = ModelConfig(
    vocab_size=32000,
    hidden_size=4096,
    intermediate_size=11008,
    num_hidden_layers=32,
    num_attention_heads=32,
    num_key_value_heads=8,  # ä½¿ç”¨ GQA (32/8 = 4 groups)
    max_position_embeddings=4096,
    rope_config=RoPEConfig(
        rope_type='yarn',   # ä½¿ç”¨ YaRN æ”¯æŒé•¿æ–‡æœ¬
        rope_theta=10000.0,
        factor=8.0          # æ‰©å±•ç³»æ•°
    )
)

model = LlmModel(config)
print(model)

# å‰å‘ä¼ æ’­ç¤ºä¾‹
input_ids = torch.randint(0, 32000, (1, 128))
output = model(input_ids)
print(f"Logits shape: {output['logits'].shape}")

```

### 2. åˆå§‹åŒ– VLM æ¨¡å‹

``` Python
from llm_model import VlmModel, VLMConfig

def dummy_vision_tower(images):
    # æ¨¡æ‹Ÿè§†è§‰ç¼–ç å™¨è¾“å‡º: (batch, num_patches, vision_hidden)
    return torch.randn(images.shape[0], 256, 1024)

vlm_config = VLMConfig(
    vocab_size=32000,
    hidden_size=4096,
    intermediate_size=11008,
    num_hidden_layers=32,
    num_attention_heads=32,
    num_key_value_heads=32,
    max_position_embeddings=2048,
    # è§†è§‰éƒ¨åˆ†é…ç½®
    image_tok=32001,        # ç‰¹æ®Š Image Token ID
    image_size=336,
    patch_size=14,
    tokens_per_image=576,   # æŠ•å½±åçš„ token æ•°é‡
    vision_hidden_size=1024,
    vision_tower=dummy_vision_tower
)

vlm_model = VlmModel(vlm_config)

```

## âš™ï¸ é…ç½®è¯´æ˜ (Configuration)

### ModelConfig (LLM)

| **å‚æ•°**                     | **ç±»å‹** | **è¯´æ˜**                                          |
| :------------------------- | :----- | :---------------------------------------------- |
| `vocab_size`               | int    | è¯è¡¨å¤§å°                                            |
| `hidden_size`              | int    | éšè—å±‚ç»´åº¦                                           |
| `intermediate_size`        | int    | MLP ä¸­é—´å±‚ç»´åº¦                                       |
| `num_hidden_layers`        | int    | Transformer å±‚æ•°                                  |
| `num_attention_heads`      | int    | æ³¨æ„åŠ›å¤´æ•° (Query)                                   |
| `num_key_value_heads`      | int    | KV å¤´æ•° (ç”¨äº GQA/MQA)                              |
| `max_position_embeddings`  | int    | æœ€å¤§åºåˆ—é•¿åº¦                                          |
| `attention_implementation` | str    | Attention å®ç°: `auto`, `sdpa` (Flash), `default` |
| `use_qk_norm`              | bool   | æ˜¯å¦å¯¹ Q/K è¿›è¡Œ LayerNorm (æ¨è True ä»¥ç¨³å®šè®­ç»ƒ)            |

### RoPEConfig (ä½ç½®ç¼–ç )

| **å‚æ•°**                    | **ç±»å‹** | **é»˜è®¤å€¼**   | **è¯´æ˜**                                 |
| :------------------------ | :----- | :-------- | :------------------------------------- |
| `rope_type`               | str    | `default` | å¯é€‰: `default`, `dynamic` (NTK), `yarn` |
| `rope_theta`              | float  | 10000.0   | RoPE çš„åŸºé¢‘                               |
| `factor`                  | float  | 1.0       | ç¼©æ”¾ç³»æ•° (ç”¨äºå¤–æ¨)                            |
| `beta_fast` / `beta_slow` | float  | -         | YaRN ç®—æ³•ä¸“ç”¨çš„æ’å€¼å‚æ•°                         |

### MoEConfig (æ··åˆä¸“å®¶)

å½“é…ç½®äº† `moe_config` æ—¶ï¼Œæ¨¡å‹ä¼šå°† MLP å±‚æ›¿æ¢ä¸º MoE å±‚ï¼ˆåœ¨ `n_dense_layer` å±‚ä¹‹åï¼‰ã€‚

| **å‚æ•°**                | **ç±»å‹** | **è¯´æ˜**                   |
| :-------------------- | :----- | :----------------------- |
| `num_experts_per_tok` | int    | æ¯ä¸ª Token æ¿€æ´»çš„ä¸“å®¶æ•°é‡ (Top-K) |
| `n_routed_experts`    | int    | è·¯ç”±ä¸“å®¶æ€»æ•°                   |
| `n_shared_experts`    | int    | å…±äº«ä¸“å®¶æ•°é‡ (æ€»æ˜¯æ¿€æ´»)            |
| `intermediate_size`   | int    | å•ä¸ªä¸“å®¶çš„ç»´åº¦                  |
| `seq_aux`             | bool   | æ˜¯å¦è®¡ç®—åºåˆ—çº§è¾…åŠ©æŸå¤±              |

## ğŸ“‚ é¡¹ç›®ç»“æ„

Plaintext

```
llm_model/
â”œâ”€â”€ llm_model.py       # æ ¸å¿ƒæ¨¡å‹å®ç° (LlmModel, DecoderLayer, Attention)
â”œâ”€â”€ vlm_model.py       # è§†è§‰è¯­è¨€æ¨¡å‹æ‰©å±• (VlmModel)
â”œâ”€â”€ sparse_moe.py      # MoE è·¯ç”±ä¸ä¸“å®¶å®ç°
â”œâ”€â”€ rope.py            # æ—‹è½¬ä½ç½®ç¼–ç  (RoPE, YaRN, Dynamic)
â”œâ”€â”€ kv_cache.py        # æ¨ç† KV ç¼“å­˜ç®¡ç†
â”œâ”€â”€ attention_masks.py # å› æœä¸ Padding Mask å¤„ç†
â””â”€â”€ model_config.py    # é…ç½®ç±»å®šä¹‰

```
